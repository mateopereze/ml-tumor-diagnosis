{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Tumor Diagnosis\n",
    "\n",
    "In this project, we aim to apply data analysis, dimensionality reduction, and clustering techniques to better understand the tumor characteristics and their classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### 01. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation and analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "import math\n",
    "\n",
    "# Modeling\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, \\\n",
    "    classification_report, confusion_matrix\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "# Visualizations\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set option to display all columns\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 02. Import data\n",
    "\n",
    "__Class distribution:__\n",
    "- __diagnosis__: Target column containing the class $labels$\n",
    "  - $M$ - $Malignant$ $\\rightarrow$ Tending to invade normal tissue, indicating a more harmful nature.\n",
    "  - $B$ - $Benign$ $\\rightarrow$ Not harmful, indicating a non-invasive and less concerning form.\n",
    "\n",
    "__Columns in the dataset:__\n",
    "\n",
    "0. __id__ Contains unique identifiers for each record. As a unique identifier, it cannot be used for classification purposes.\n",
    "1. __radius__ Mean of distances from the center to points on the perimeter of the nucleus.\n",
    "2. __texture__ Standard deviation of the gray-scale values.\n",
    "3. __perimeter__ Total distance around the boundary of the nucleus.\n",
    "4. __area__ Total area of the nucleus.\n",
    "5. __smoothness__ Local variation in radius lengths, indicating the smoothness of the boundary.\n",
    "6. __compactness__ Indicating how compact the nucleus is. (perimeter^2 / area - 1.0) \n",
    "7. __concavity__ Severity of concave portions of the contour, measuring how inward the boundary is.\n",
    "8. __concave points__ Number of concave portions on the contour of the nucleus.\n",
    "9. __symmetry__ Measures the symmetry of the nucleus.\n",
    "10. __fractal dimension__ A measure of the \"coastline approximation\", calculated as the ratio of the perimeter to area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find principal path of the project\n",
    "from pathlib import Path\n",
    "project_root = str(Path.cwd().parents[0])\n",
    "\n",
    "# Load the column names from the JSON file\n",
    "with open(project_root + '.\\static\\column_names.json', 'r') as json_file:\n",
    "    saved_column_names = json.load(json_file)\n",
    "\n",
    "# Read the .data file into a Pandas DataFrame\n",
    "df = pd.read_csv(project_root + '.\\static\\wdbc.data', header=None, names=saved_column_names)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beautifully formatted output for dataset details\n",
    "print(f\"Data Points (Rows)   : {df.shape[0]:,}\")\n",
    "print(f\"Features (Columns)   : {df.shape[1]}\")\n",
    "print(f\"Feature Names        : {', '.join(df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 03. Data preprocesing\n",
    "\n",
    "In this step, the unique identifier column will be removed from the dataset, as it does not contribute to the classification task. Additionally, we will perform label encoding on the $diagnosis$ column to transform the categorical labels into numerical values, making them suitable for machine learning models.\n",
    "\n",
    "The LabelEncoder from sklearn.preprocessing will be used to convert the labels in the diagnosis column ('B' for Benign and 'M' for Malignant) to binary values (0 and 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 'id' column is an arbitrary identifier with no meaningful contribution \n",
    "# to pattern analysis, correlations, or clustering, and may introduce noise into the model.\n",
    "df_cleaned = df.drop(columns=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an instance of LabelEncoder to perform label encoding\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Convert 'diagnosis' column from 'b' and 'm' to 0 and 1\n",
    "df_cleaned['diagnosis'] = label_encoder.fit_transform(df_cleaned['diagnosis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See preprocessing result\n",
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 04. Correlation Analysis\n",
    "\n",
    "The purpose of this analysis is to examine the correlation between the features in the dataset. Identifying highly correlated features is important because multicollinearity can negatively impact model performance. However, in this case, we will not remove any features, as we want to evaluate their influence on the performance of PCA (Principal Component Analysis) and clustering in later stages of the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create correlation matrix\n",
    "corr_matrix = df_cleaned.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Pairplot from Seaborn to see relationship between individual features and diagnosis\n",
    "# 'Benign (0)', 'Malignant (1)'\n",
    "sns.pairplot(df_cleaned, palette='coolwarm', hue='diagnosis')\n",
    "\n",
    "# Adjust legend position to the upper left corner\n",
    "plt.legend(title='Diagnosis', loc='upper left', labels=['Benign (0)', 'Malignant (1)'], fontsize=12)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of Features\n",
    "plt.figure(figsize=(19, 17))\n",
    "\n",
    "# Automatically get numerical columns\n",
    "numerical_features = df_cleaned.select_dtypes(include=['number']).columns\n",
    "\n",
    "# Number of rows and columns for subplots\n",
    "rows = math.ceil(len(df_cleaned.columns) / 3)\n",
    "cols = 3\n",
    "\n",
    "# Adjust the figure size\n",
    "fig, ax = plt.subplots(nrows=rows, ncols=cols, figsize=(15, 15))\n",
    "\n",
    "# Flatten the axis array for easy access\n",
    "ax = ax.flatten()\n",
    "\n",
    "# Iterate over the numerical features\n",
    "for i in range(len(numerical_features)):\n",
    "    sns.histplot(df_cleaned[numerical_features[i]], color='crimson', kde=True, ax=ax[i])\n",
    "    ax[i].set_title(f'Distribution: {numerical_features[i]}')\n",
    "\n",
    "# Remove unused subplots if there are fewer features\n",
    "for j in range(i + 1, rows * cols):\n",
    "    fig.delaxes(ax[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the correlation matrix\n",
    "plt.figure(figsize=(19, 17))\n",
    "sns.heatmap(corr_matrix, xticklabels=corr_matrix.columns, yticklabels=corr_matrix.columns, annot=True, annot_kws={\"size\": 8}, fmt=\".2f\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask the diagonal (set to NaN) to avoid self-correlation\n",
    "np.fill_diagonal(corr_matrix.values, np.nan)\n",
    "\n",
    "# Stack the correlation matrix and sort the values\n",
    "corr_pairs = corr_matrix.unstack().sort_values(ascending=False)\n",
    "\n",
    "# Show the pairs with the highest correlation, excluding the diagonal\n",
    "corr_pairs.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 05. Decision Tree\n",
    "\n",
    "Decision Trees are a supervised classification and regression algorithm that splits the dataset into subsets based on feature values. It recursively partitions the data, creating a tree-like structure where each internal node represents a feature test, each branch represents the outcome of the test, and each leaf node represents a predicted class or value.\n",
    "\n",
    "- __Input Features:__ Decision trees work well with both numerical and categorical features. The model recursively splits the data based on feature values to create the tree structure. The most informative features (based on some splitting criterion) are used at the top of the tree.\n",
    "\n",
    "- __Splitting Criterion:__ Decision trees use criteria such as Gini impurity or Entropy (for classification) to evaluate how well a feature splits the data. A split is made when it minimizes impurity in the child nodes.\n",
    "\n",
    "    - Gini Impurity: Measures the likelihood of incorrect classification of a new instance.\n",
    "    \n",
    "    \\$\\Gini = 1 - \\sum_{i=1}^{C} p_i^2$\n",
    "\n",
    "        where \\$\\p_i$ is the probability of class \\$\\i$ in the current node.\n",
    "\n",
    "    - Entropy: Measures the amount of uncertainty or disorder in the data.\n",
    "    \n",
    "    \\$\\Entropy = - \\sum_{i=1}^{C} p_i\\log_2(p_i)$\n",
    "\n",
    "        where \\$\\p_i$ is the probability of class \\$\\i$ in the current node.\n",
    "\n",
    "- __Thresholding:__ Once the model outputs probabilities, a threshold (commonly 0.5) is applied to classify observations into one of the two categories.\n",
    "\n",
    "\n",
    "Combining Trees with Other Models:\n",
    "If you plan to compare decision trees with models like Logistic Regression, SVMs, or Gradient Descent-based models, standardizing ensures that all models work on similarly scaled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features and target\n",
    "X = df_cleaned.iloc[:, 1:]  # Features (skip diagnosis)\n",
    "y = df_cleaned['diagnosis']  # Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The baseline establishes an initial performance level that serves as a minimum standard for comparing more complex models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model with default parameters\n",
    "dt_baseline = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Train the model\n",
    "dt_baseline.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = dt_baseline.predict(X_test)\n",
    "y_pred_prob = dt_baseline.predict_proba(X_test)[:, 1]  # Probabilities for ROC-AUC\n",
    "\n",
    "# Calculate Evaluation Metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "\n",
    "# Print Metrics\n",
    "print(\"Baseline Decision Tree Performance:\")\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n",
    "print(f\"ROC-AUC: {roc_auc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the tree\n",
    "plt.figure(figsize=(9, 7))\n",
    "plot_tree(dt_baseline, feature_names=X.columns, class_names=[\"Benign\", \"Malignant\"], filled=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 06. Evaluate the Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters to tune\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 10, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_features': [None, 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Grid Search with 5-fold cross-validation\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=DecisionTreeClassifier(random_state=42),\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and model\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the best model\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_pred_prob = best_model.predict_proba(X_test)[:, 1]  # Probability for the positive class\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "\n",
    "# Print metrics\n",
    "print(\"\\n# Evaluation Metrics for Tuned Decision Tree:\")\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n",
    "print(f\"ROC-AUC: {roc_auc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 07. Compare with Other Models (\"Fight of Models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models\n",
    "models = {\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42, **best_params),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(random_state=42),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42)\n",
    "}\n",
    "\n",
    "# Train and evaluate models\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_prob = model.predict_proba(X_test)[:, 1]  # Get probabilities for ROC-AUC\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "\n",
    "    # Store results\n",
    "    results[name] = {\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1 Score\": f1,\n",
    "        \"ROC-AUC\": roc_auc\n",
    "    }\n",
    "\n",
    "    # Print results\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Accuracy: {accuracy:.2f}\")\n",
    "    print(f\"  Precision: {precision:.2f}\")\n",
    "    print(f\"  Recall: {recall:.2f}\")\n",
    "    print(f\"  F1 Score: {f1:.2f}\")\n",
    "    print(f\"  ROC-AUC: {roc_auc:.2f}\")\n",
    "    print()\n",
    "\n",
    "# Convert results to DataFrame for better visualization\n",
    "results_df = pd.DataFrame(results).T\n",
    "\n",
    "# Plot results for Accuracy, Precision, Recall, F1, and ROC-AUC\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot Accuracy\n",
    "axes[0, 0].bar(results_df.index, results_df['Accuracy'], color='skyblue')\n",
    "axes[0, 0].set_title('Accuracy')\n",
    "axes[0, 0].set_ylabel('Score')\n",
    "\n",
    "# Plot Precision\n",
    "axes[0, 1].bar(results_df.index, results_df['Precision'], color='lightcoral')\n",
    "axes[0, 1].set_title('Precision')\n",
    "axes[0, 1].set_ylabel('Score')\n",
    "\n",
    "# Plot Recall\n",
    "axes[1, 0].bar(results_df.index, results_df['Recall'], color='lightgreen')\n",
    "axes[1, 0].set_title('Recall')\n",
    "axes[1, 0].set_ylabel('Score')\n",
    "\n",
    "# Plot F1 Score\n",
    "axes[1, 1].bar(results_df.index, results_df['F1 Score'], color='orange')\n",
    "axes[1, 1].set_title('F1 Score')\n",
    "axes[1, 1].set_ylabel('Score')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 09. Model evaluation and interpretation\n",
    "\n",
    "In this step, we will evaluate the performance of the decision tree model by examining its ability to classify diagnoses accurately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n",
    "print(f\"ROC-AUC: {roc_auc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the confusion matrix as a heatmap\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='g', cmap='Blues', xticklabels=['Benign', 'Malignant'], yticklabels=['Benign', 'Malignant'], cbar=False)\n",
    "plt.title('Confusion Matrix: Logistic Regression vs Diagnosis', fontsize=14)\n",
    "plt.xlabel('Predicted Diagnosis', fontsize=12)\n",
    "plt.ylabel('True Diagnosis', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label=f\"Logistic Regression (AUC = {roc_auc:.2f})\", color=\"blue\")\n",
    "plt.plot([0, 1], [0, 1], color=\"gray\", linestyle=\"--\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Metrics__\n",
    "- Accuracy (0.97): The model correctly classifies 97% of the instances. This is a strong indication of its overall reliability in distinguishing between malignant and benign cases.\n",
    "\n",
    "- Precision (0.98): Out of all the cases the model predicted as malignant, 98% were indeed malignant. This metric highlights the model's ability to avoid false positives (incorrectly predicting benign cases as malignant).\n",
    "\n",
    "- Recall (0.95): The model successfully identified 95% of the actual malignant cases, which shows its capability to minimize false negatives (failing to detect malignant cases).\n",
    "\n",
    "- F1 Score (0.96): The F1 score balances precision and recall, providing a harmonic mean. A score of 0.96 confirms the model's robustness in handling both false positives and false negatives.\n",
    "\n",
    "- ROC-AUC (1.00): The model achieved perfect separation between the malignant and benign classes, demonstrating its exceptional ability to differentiate between the two.\n",
    "\n",
    "\n",
    "__Conclusion__\n",
    "\n",
    "In summary, __false positives (FP)__ and __false negatives (FN)__ are critical in cancer diagnosis because they have direct implications on patient health and treatment. Managing these cases requires thorough review, additional testing, continuous monitoring, and refining the models to minimize these errors. Collaborating with medical specialists and integrating advanced diagnostic technologies are essential to improving accuracy and reducing the risks associated with diagnostic errors.\n",
    "\n",
    "The modelâ€™s performance underscores the effectiveness of logistic regression in this context, demonstrating its potential as a reliable tool for aiding in cancer diagnosis. However, validation with domain experts and real-world data is essential to ensure clinical applicability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
